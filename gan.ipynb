{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Definir el Generador\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf*8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf*8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Definir el Discriminador\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc, ndf):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf*4, ndf*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf*8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Función para inicializar los pesos de la red\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuración de parámetros\n",
    "nz = 100    # Tamaño del vector de ruido de entrada al generador\n",
    "ngf = 64    # Número de filtros en la última capa de desconvolución del generador\n",
    "ndf = 64    # Número de filtros en la primera capa de convolución del discriminador\n",
    "nc = 3      # Número de canales de la imagen (RGB)\n",
    "lr = 0.0002 # Tasa de aprendizaje\n",
    "beta1 = 0.5 # Beta1 para el optimizador Adam\n",
    "\n",
    "# Crear modelos\n",
    "netG = Generator(nz, ngf, nc).to(device)\n",
    "netD = Discriminator(nc, ndf).to(device)\n",
    "\n",
    "# Inicializar pesos\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Configurar optimizadores y criterio de pérdida\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Función para entrenar el GAN y generar imágenes\n",
    "def train_gan(num_epochs, dataloader, device, netG, netD, optimizerG, optimizerD):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            \n",
    "            # (1) Actualizar el Discriminador: maximizar log(D(x)) + log(1 - D(G(z)))\n",
    "            \n",
    "            netD.zero_grad()\n",
    "            real_cpu = data[0].to(device)\n",
    "            b_size = real_cpu.size(0)\n",
    "            label = torch.full((b_size,), 1, dtype=torch.float, device=device)\n",
    "            output = netD(real_cpu).view(-1)\n",
    "            errD_real = criterion(output, label)\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "            fake = netG(noise)\n",
    "            label.fill_(0)\n",
    "            output = netD(fake.detach()).view(-1)\n",
    "            errD_fake = criterion(output, label)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            errD = errD_real + errD_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "            \n",
    "            # (2) Actualizar el Generador: maximizar log(D(G(z)))\n",
    "            \n",
    "            netG.zero_grad()\n",
    "            label.fill_(1)\n",
    "            output = netD(fake).view(-1)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Imprimir estadísticas del entrenamiento\n",
    "            if i % 50 == 0:\n",
    "                print(f'Epoch [{epoch}/{num_epochs}], Batch [{i}/{len(dataloader)}], '\n",
    "                      f'Loss_D: {errD.item():.4f}, Loss_G: {errG.item():.4f}, '\n",
    "                      f'D(x): {D_x:.4f}, D(G(z)): {D_G_z1:.4f} / {D_G_z2:.4f}')\n",
    "\n",
    "        # Guardar imágenes generadas al final de cada época\n",
    "        with torch.no_grad():\n",
    "            fake = netG(torch.randn(64, nz, 1, 1, device=device))\n",
    "            torchvision.utils.save_image(fake, f'generated_images/fake_samples_epoch_{epoch+1:03d}.png', normalize=True)\n",
    "\n",
    "# Dataset de imágenes de melanomas\n",
    "dataset_path = './train'  # Ruta al dataset de melanomas\n",
    "\n",
    "# Transformaciones para el dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10], Batch [0/372], Loss_D: 1.7514, Loss_G: 3.2672, D(x): 0.3274, D(G(z)): 0.2883 / 0.0419\n",
      "Epoch [0/10], Batch [50/372], Loss_D: 0.2446, Loss_G: 13.5027, D(x): 0.9774, D(G(z)): 0.1838 / 0.0000\n",
      "Epoch [0/10], Batch [100/372], Loss_D: 0.0629, Loss_G: 35.4192, D(x): 0.9453, D(G(z)): 0.0000 / 0.0000\n",
      "Epoch [0/10], Batch [150/372], Loss_D: 0.2375, Loss_G: 12.1313, D(x): 0.8915, D(G(z)): 0.0001 / 0.0000\n",
      "Epoch [0/10], Batch [200/372], Loss_D: 0.0687, Loss_G: 2.6160, D(x): 0.9956, D(G(z)): 0.0603 / 0.1123\n",
      "Epoch [0/10], Batch [250/372], Loss_D: 0.0730, Loss_G: 4.0820, D(x): 0.9663, D(G(z)): 0.0368 / 0.0263\n",
      "Epoch [0/10], Batch [300/372], Loss_D: 0.2931, Loss_G: 8.4703, D(x): 0.8696, D(G(z)): 0.0332 / 0.0004\n",
      "Epoch [0/10], Batch [350/372], Loss_D: 0.0767, Loss_G: 5.5549, D(x): 0.9466, D(G(z)): 0.0156 / 0.0061\n",
      "Epoch [1/10], Batch [0/372], Loss_D: 1.2431, Loss_G: 2.7218, D(x): 0.4507, D(G(z)): 0.0017 / 0.2412\n",
      "Epoch [1/10], Batch [50/372], Loss_D: 2.1859, Loss_G: 1.8297, D(x): 0.5765, D(G(z)): 0.3844 / 0.3434\n",
      "Epoch [1/10], Batch [100/372], Loss_D: 0.2480, Loss_G: 4.1657, D(x): 0.9074, D(G(z)): 0.1175 / 0.0211\n",
      "Epoch [1/10], Batch [150/372], Loss_D: 0.1114, Loss_G: 4.5965, D(x): 0.9457, D(G(z)): 0.0508 / 0.0148\n",
      "Epoch [1/10], Batch [200/372], Loss_D: 0.2403, Loss_G: 3.2590, D(x): 0.8404, D(G(z)): 0.0480 / 0.0554\n",
      "Epoch [1/10], Batch [250/372], Loss_D: 0.5102, Loss_G: 2.8506, D(x): 0.7603, D(G(z)): 0.1274 / 0.0721\n",
      "Epoch [1/10], Batch [300/372], Loss_D: 0.3785, Loss_G: 4.0213, D(x): 0.7535, D(G(z)): 0.0239 / 0.0237\n",
      "Epoch [1/10], Batch [350/372], Loss_D: 0.6087, Loss_G: 2.4930, D(x): 0.6342, D(G(z)): 0.0442 / 0.1077\n",
      "Epoch [2/10], Batch [0/372], Loss_D: 1.3999, Loss_G: 4.2858, D(x): 0.9964, D(G(z)): 0.6694 / 0.0225\n",
      "Epoch [2/10], Batch [50/372], Loss_D: 0.6840, Loss_G: 5.2322, D(x): 0.9140, D(G(z)): 0.4110 / 0.0090\n",
      "Epoch [2/10], Batch [100/372], Loss_D: 1.0350, Loss_G: 3.3827, D(x): 0.8673, D(G(z)): 0.4767 / 0.0679\n",
      "Epoch [2/10], Batch [150/372], Loss_D: 2.4951, Loss_G: 0.2848, D(x): 0.1037, D(G(z)): 0.0536 / 0.7753\n",
      "Epoch [2/10], Batch [200/372], Loss_D: 0.5655, Loss_G: 4.1099, D(x): 0.9066, D(G(z)): 0.3348 / 0.0271\n",
      "Epoch [2/10], Batch [250/372], Loss_D: 0.4409, Loss_G: 3.6115, D(x): 0.6867, D(G(z)): 0.0300 / 0.0474\n",
      "Epoch [2/10], Batch [300/372], Loss_D: 1.0267, Loss_G: 1.9245, D(x): 0.4666, D(G(z)): 0.0729 / 0.2037\n",
      "Epoch [2/10], Batch [350/372], Loss_D: 1.2805, Loss_G: 2.9525, D(x): 0.8925, D(G(z)): 0.6074 / 0.0829\n",
      "Epoch [3/10], Batch [0/372], Loss_D: 0.4970, Loss_G: 1.8923, D(x): 0.8581, D(G(z)): 0.2508 / 0.2293\n",
      "Epoch [3/10], Batch [50/372], Loss_D: 0.8737, Loss_G: 2.8287, D(x): 0.9105, D(G(z)): 0.4753 / 0.0735\n",
      "Epoch [3/10], Batch [100/372], Loss_D: 0.7386, Loss_G: 2.5152, D(x): 0.7983, D(G(z)): 0.3603 / 0.1073\n",
      "Epoch [3/10], Batch [150/372], Loss_D: 0.5533, Loss_G: 2.4552, D(x): 0.7769, D(G(z)): 0.2258 / 0.1044\n",
      "Epoch [3/10], Batch [200/372], Loss_D: 1.3899, Loss_G: 2.4066, D(x): 0.7596, D(G(z)): 0.6070 / 0.1351\n",
      "Epoch [3/10], Batch [250/372], Loss_D: 1.0436, Loss_G: 3.7111, D(x): 0.9687, D(G(z)): 0.5673 / 0.0346\n",
      "Epoch [3/10], Batch [300/372], Loss_D: 1.0150, Loss_G: 2.1152, D(x): 0.7448, D(G(z)): 0.4383 / 0.2245\n",
      "Epoch [3/10], Batch [350/372], Loss_D: 1.4134, Loss_G: 3.1961, D(x): 0.9564, D(G(z)): 0.6844 / 0.0616\n",
      "Epoch [4/10], Batch [0/372], Loss_D: 1.2650, Loss_G: 1.9004, D(x): 0.8426, D(G(z)): 0.5663 / 0.1980\n",
      "Epoch [4/10], Batch [50/372], Loss_D: 0.7196, Loss_G: 2.8007, D(x): 0.9078, D(G(z)): 0.4302 / 0.0740\n",
      "Epoch [4/10], Batch [100/372], Loss_D: 0.7929, Loss_G: 1.7380, D(x): 0.6439, D(G(z)): 0.2554 / 0.2147\n",
      "Epoch [4/10], Batch [150/372], Loss_D: 1.0355, Loss_G: 1.3640, D(x): 0.4828, D(G(z)): 0.1188 / 0.2936\n",
      "Epoch [4/10], Batch [200/372], Loss_D: 1.0200, Loss_G: 3.1666, D(x): 0.4480, D(G(z)): 0.0219 / 0.0775\n",
      "Epoch [4/10], Batch [250/372], Loss_D: 0.7530, Loss_G: 2.0630, D(x): 0.6156, D(G(z)): 0.2034 / 0.1573\n",
      "Epoch [4/10], Batch [300/372], Loss_D: 1.3430, Loss_G: 0.9878, D(x): 0.3755, D(G(z)): 0.1968 / 0.4120\n",
      "Epoch [4/10], Batch [350/372], Loss_D: 1.3591, Loss_G: 1.4818, D(x): 0.3891, D(G(z)): 0.1881 / 0.3350\n",
      "Epoch [5/10], Batch [0/372], Loss_D: 1.1293, Loss_G: 1.1180, D(x): 0.5001, D(G(z)): 0.1737 / 0.4056\n",
      "Epoch [5/10], Batch [50/372], Loss_D: 0.7707, Loss_G: 3.5740, D(x): 0.8896, D(G(z)): 0.4592 / 0.0325\n",
      "Epoch [5/10], Batch [100/372], Loss_D: 1.2470, Loss_G: 0.8335, D(x): 0.3870, D(G(z)): 0.1495 / 0.5874\n",
      "Epoch [5/10], Batch [150/372], Loss_D: 1.3358, Loss_G: 2.1692, D(x): 0.6340, D(G(z)): 0.5305 / 0.1270\n",
      "Epoch [5/10], Batch [200/372], Loss_D: 0.9864, Loss_G: 1.3181, D(x): 0.5737, D(G(z)): 0.2866 / 0.3124\n",
      "Epoch [5/10], Batch [250/372], Loss_D: 0.4006, Loss_G: 3.1249, D(x): 0.8238, D(G(z)): 0.1721 / 0.0694\n",
      "Epoch [5/10], Batch [300/372], Loss_D: 1.3169, Loss_G: 1.1005, D(x): 0.5792, D(G(z)): 0.4355 / 0.3706\n",
      "Epoch [5/10], Batch [350/372], Loss_D: 0.7831, Loss_G: 1.6087, D(x): 0.7182, D(G(z)): 0.3077 / 0.2428\n",
      "Epoch [6/10], Batch [0/372], Loss_D: 0.9486, Loss_G: 2.2108, D(x): 0.8028, D(G(z)): 0.3924 / 0.1287\n",
      "Epoch [6/10], Batch [50/372], Loss_D: 1.0547, Loss_G: 1.4036, D(x): 0.5884, D(G(z)): 0.3273 / 0.3036\n",
      "Epoch [6/10], Batch [100/372], Loss_D: 1.3225, Loss_G: 1.4041, D(x): 0.3307, D(G(z)): 0.0660 / 0.4038\n",
      "Epoch [6/10], Batch [150/372], Loss_D: 1.0095, Loss_G: 2.4412, D(x): 0.7695, D(G(z)): 0.4469 / 0.1414\n",
      "Epoch [6/10], Batch [200/372], Loss_D: 1.1282, Loss_G: 1.2965, D(x): 0.6554, D(G(z)): 0.3717 / 0.3077\n",
      "Epoch [6/10], Batch [250/372], Loss_D: 0.8926, Loss_G: 1.2721, D(x): 0.5187, D(G(z)): 0.1472 / 0.3265\n",
      "Epoch [6/10], Batch [300/372], Loss_D: 0.8600, Loss_G: 1.7280, D(x): 0.6424, D(G(z)): 0.3092 / 0.2276\n",
      "Epoch [6/10], Batch [350/372], Loss_D: 1.4575, Loss_G: 2.3309, D(x): 0.6534, D(G(z)): 0.5597 / 0.1305\n",
      "Epoch [7/10], Batch [0/372], Loss_D: 1.5227, Loss_G: 2.7145, D(x): 0.8391, D(G(z)): 0.6817 / 0.0804\n",
      "Epoch [7/10], Batch [50/372], Loss_D: 1.2417, Loss_G: 1.1803, D(x): 0.4300, D(G(z)): 0.2508 / 0.3689\n",
      "Epoch [7/10], Batch [100/372], Loss_D: 1.1053, Loss_G: 0.9510, D(x): 0.5606, D(G(z)): 0.3503 / 0.4504\n",
      "Epoch [7/10], Batch [150/372], Loss_D: 0.9943, Loss_G: 2.0737, D(x): 0.4767, D(G(z)): 0.1733 / 0.1537\n",
      "Epoch [7/10], Batch [200/372], Loss_D: 1.3717, Loss_G: 0.8872, D(x): 0.4193, D(G(z)): 0.2881 / 0.4785\n",
      "Epoch [7/10], Batch [250/372], Loss_D: 0.9748, Loss_G: 1.9840, D(x): 0.8235, D(G(z)): 0.5069 / 0.1590\n",
      "Epoch [7/10], Batch [300/372], Loss_D: 1.3719, Loss_G: 1.8656, D(x): 0.4829, D(G(z)): 0.3876 / 0.2001\n",
      "Epoch [7/10], Batch [350/372], Loss_D: 1.4801, Loss_G: 1.5856, D(x): 0.4328, D(G(z)): 0.2502 / 0.2410\n",
      "Epoch [8/10], Batch [0/372], Loss_D: 0.9563, Loss_G: 2.2027, D(x): 0.7685, D(G(z)): 0.4597 / 0.1294\n",
      "Epoch [8/10], Batch [50/372], Loss_D: 1.4758, Loss_G: 1.3993, D(x): 0.6200, D(G(z)): 0.5515 / 0.2735\n",
      "Epoch [8/10], Batch [100/372], Loss_D: 0.9698, Loss_G: 1.1077, D(x): 0.6114, D(G(z)): 0.3156 / 0.3615\n",
      "Epoch [8/10], Batch [150/372], Loss_D: 1.0332, Loss_G: 1.6647, D(x): 0.7465, D(G(z)): 0.4556 / 0.2285\n",
      "Epoch [8/10], Batch [200/372], Loss_D: 0.7845, Loss_G: 2.0210, D(x): 0.6680, D(G(z)): 0.2729 / 0.1544\n",
      "Epoch [8/10], Batch [250/372], Loss_D: 0.7661, Loss_G: 1.6957, D(x): 0.7820, D(G(z)): 0.3741 / 0.2311\n",
      "Epoch [8/10], Batch [300/372], Loss_D: 2.3234, Loss_G: 1.6420, D(x): 0.5709, D(G(z)): 0.7528 / 0.2260\n",
      "Epoch [8/10], Batch [350/372], Loss_D: 1.3306, Loss_G: 1.7179, D(x): 0.6153, D(G(z)): 0.4939 / 0.2118\n",
      "Epoch [9/10], Batch [0/372], Loss_D: 1.3214, Loss_G: 1.1090, D(x): 0.4700, D(G(z)): 0.2806 / 0.3556\n",
      "Epoch [9/10], Batch [50/372], Loss_D: 0.9935, Loss_G: 1.6836, D(x): 0.5284, D(G(z)): 0.2641 / 0.2010\n",
      "Epoch [9/10], Batch [100/372], Loss_D: 0.6416, Loss_G: 2.0171, D(x): 0.7639, D(G(z)): 0.2786 / 0.1543\n",
      "Epoch [9/10], Batch [150/372], Loss_D: 1.6932, Loss_G: 1.0044, D(x): 0.4394, D(G(z)): 0.4667 / 0.4245\n",
      "Epoch [9/10], Batch [200/372], Loss_D: 1.2413, Loss_G: 1.3772, D(x): 0.4718, D(G(z)): 0.3181 / 0.3220\n",
      "Epoch [9/10], Batch [250/372], Loss_D: 1.5520, Loss_G: 1.0417, D(x): 0.3173, D(G(z)): 0.2268 / 0.3971\n",
      "Epoch [9/10], Batch [300/372], Loss_D: 1.1848, Loss_G: 1.7414, D(x): 0.6563, D(G(z)): 0.4589 / 0.2007\n",
      "Epoch [9/10], Batch [350/372], Loss_D: 0.8076, Loss_G: 1.9922, D(x): 0.5531, D(G(z)): 0.1123 / 0.1684\n",
      "Entrenamiento del GAN completado\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Crear DataLoader para el dataset de entrenamiento\n",
    "batch_size = 32\n",
    "dataset = torchvision.datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# Entrenar el GAN y guardar imágenes generadas\n",
    "num_epochs = 10\n",
    "train_gan(num_epochs, dataloader, device, netG, netD, optimizerG, optimizerD)\n",
    "\n",
    "print('Entrenamiento del GAN completado')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
